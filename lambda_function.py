#!/usr/bin/env python3
"""
vd-speed-test aggregator Lambda
- Hourly: aggregates 15-min records into hourly summary, writes to:
    s3://<S3_BUCKET_HOURLY>/aggregated/year=YYYY/month=YYYYMM/day=YYYYMMDD/hour=YYYYMMDDHH/speed_test_summary.json
- Daily: aggregates minute-level results into day summary, writes to:
    s3://<S3_BUCKET>/aggregated/year=YYYY/month=YYYYMM/day=YYYYMMDD/speed_test_summary.json
- Weekly/Monthly/Yearly: roll-ups from daily â†’ weekly/monthly/yearly, writing into:
    s3://<S3_BUCKET_WEEKLY>/aggregated/year=YYYY/week=YYYYWWW/speed_test_summary.json
    s3://<S3_BUCKET_MONTHLY>/aggregated/year=YYYY/month=YYYYMM/speed_test_summary.json
    s3://<S3_BUCKET_YEARLY>/aggregated/year=YYYY/speed_test_summary.json
    
Note: All aggregation levels now use the same filename (speed_test_summary.json) for simplicity.
"""

import boto3
import json
import datetime
import pytz
import logging
import os
import sys
from logging.handlers import RotatingFileHandler
from statistics import mean, median
from collections import Counter
from functools import wraps
from calendar import monthrange  # for clean month-end calculation

# --- Configuration from config.json + env vars --------------------------------
CONFIG_PATH = os.path.join(os.path.dirname(__file__), "config.json")
DEFAULT_CONFIG = {
    "s3_bucket": "vd-speed-test",
    "s3_bucket_hourly": "vd-speed-test-hourly-prod",
    "s3_bucket_weekly": "vd-speed-test-weekly-prod",
    "s3_bucket_monthly": "vd-speed-test-monthly-prod",
    "s3_bucket_yearly": "vd-speed-test-yearly-prod",
    "aws_region": "ap-south-1",
    "timezone": "Asia/Kolkata",
    "expected_speed_mbps": 200,
    "tolerance_percent": 10,
    "log_level": "INFO",
    "log_max_bytes": 10485760,
    "log_backup_count": 5
}

# Load config
config = DEFAULT_CONFIG.copy()
if os.path.exists(CONFIG_PATH):
    try:
        with open(CONFIG_PATH, "r", encoding="utf-8") as f:
            config.update(json.load(f))
    except Exception:
        pass  # Use defaults if config fails to load

S3_BUCKET = os.environ.get("S3_BUCKET", config.get("s3_bucket"))
AWS_REGION1 = os.environ.get("AWS_REGION1", config.get("aws_region"))
TIMEZONE = pytz.timezone(config.get("timezone"))
EXPECTED_SPEED_MBPS = float(os.environ.get("EXPECTED_SPEED_MBPS", str(config.get("expected_speed_mbps"))))
TOLERANCE_PERCENT = float(os.environ.get("TOLERANCE_PERCENT", str(config.get("tolerance_percent"))))

# Rollup buckets from config.json (with environment variable override)
S3_BUCKET_HOURLY = os.environ.get("S3_BUCKET_HOURLY", config.get("s3_bucket_hourly"))
S3_BUCKET_WEEKLY = os.environ.get("S3_BUCKET_WEEKLY", config.get("s3_bucket_weekly"))
S3_BUCKET_MONTHLY = os.environ.get("S3_BUCKET_MONTHLY", config.get("s3_bucket_monthly"))
S3_BUCKET_YEARLY = os.environ.get("S3_BUCKET_YEARLY", config.get("s3_bucket_yearly"))

LOG_FILE_PATH = os.path.join(os.getcwd(), "aggregator.log")
LOG_MAX_BYTES = config.get("log_max_bytes")
LOG_BACKUP_COUNT = config.get("log_backup_count")
LOG_LEVEL = os.getenv("LOG_LEVEL", config.get("log_level")).upper()
try:
    HOSTNAME = os.getenv("HOSTNAME", os.uname().nodename)
except AttributeError:
    HOSTNAME = os.getenv("HOSTNAME", "unknown-host")


# --- AWS Client ---------------------------------------------------------------
s3 = boto3.client("s3", region_name=AWS_REGION1)

# --- Custom JSON Logger -------------------------------------------------------
class CustomLogger:
    def __init__(self, name=__name__, level=LOG_LEVEL):
        self.logger = logging.getLogger(name)
        if not self.logger.handlers:
            formatter = self.JsonFormatter()

            console_handler = logging.StreamHandler(sys.stdout)
            console_handler.setFormatter(formatter)
            self.logger.addHandler(console_handler)

            if not self.is_lambda_environment():
                file_handler = RotatingFileHandler(
                    LOG_FILE_PATH, maxBytes=LOG_MAX_BYTES, backupCount=LOG_BACKUP_COUNT, encoding="utf-8"
                )
                file_handler.setFormatter(formatter)
                self.logger.addHandler(file_handler)

        self.logger.setLevel(level)
        self.logger.propagate = False

    class JsonFormatter(logging.Formatter):
        def format(self, record):
            entry = {
                "timestamp": datetime.datetime.now(datetime.UTC).isoformat() + "Z",
                "level": record.levelname,
                "message": record.getMessage(),
                "function": record.funcName,
                "module": record.module,
                "hostname": HOSTNAME,
            }
            if record.exc_info:
                entry["error"] = self.formatException(record.exc_info)
            return json.dumps(entry)

    @staticmethod
    def is_lambda_environment():
        return "AWS_LAMBDA_FUNCTION_NAME" in os.environ

    def info(self, msg, *args): self.logger.info(msg, *args)
    def warning(self, msg, *args): self.logger.warning(msg, *args)
    def error(self, msg, *args): self.logger.error(msg, *args)
    def debug(self, msg, *args): self.logger.debug(msg, *args)
    def exception(self, msg, *args): self.logger.exception(msg, *args)

log = CustomLogger(__name__)

# --- Decorator ----------------------------------------------------------------
def log_execution(func):
    @wraps(func)
    def wrapper(*args, **kwargs):
        log.info(f"Starting: {func.__name__}")
        start = datetime.datetime.now(datetime.UTC)
        try:
            result = func(*args, **kwargs)
            duration = (datetime.datetime.now(datetime.UTC) - start).total_seconds()
            log.info(f"Completed: {func.__name__} in {duration:.2f}s")
            return result
        except Exception as e:
            log.exception(f"Error in {func.__name__}: {e}")
            raise
    return wrapper

# --- Helpers ------------------------------------------------------------------
def list_objects(prefix: str):
    paginator = s3.get_paginator("list_objects_v2")
    for page in paginator.paginate(Bucket=S3_BUCKET, Prefix=prefix):
        for obj in page.get("Contents", []):
            yield obj["Key"]

def read_json(key: str) -> dict:
    obj = s3.get_object(Bucket=S3_BUCKET, Key=key)
    return json.loads(obj["Body"].read().decode("utf-8"))

def parse_float(value):
    if isinstance(value, str):
        value = value.replace("ms", "").replace("Mbps", "").strip()
    try:
        return float(value)
    except (ValueError, TypeError):
        return None

def parse_mbps(value_with_suffix):
    try:
        return float(str(value_with_suffix).strip().split()[0])
    except Exception:
        return None

def stats(values):
    if not values:
        return {}
    values_sorted = sorted(values)
    idx = lambda p: int(p * (len(values_sorted) - 1)) if len(values_sorted) > 1 else 0
    return {
        "avg": round(mean(values), 2),
        "median": round(median(values), 2),
        "max": round(max(values), 2),
        "min": round(min(values), 2),
        "p99": round(values_sorted[idx(0.99)], 2),
        "p95": round(values_sorted[idx(0.95)], 2),
        "p90": round(values_sorted[idx(0.90)], 2),
        "p50": round(values_sorted[idx(0.50)], 2),
    }

def detect_anomalies(downloads, uploads, pings):
    anomalies = []
    if downloads:
        avg_dl = mean(downloads)
        min_dl = min(downloads)
        threshold_dl = EXPECTED_SPEED_MBPS * (1 - TOLERANCE_PERCENT/100)

        if avg_dl < threshold_dl:
            anomalies.append(f"Below threshold: avg download {avg_dl:.2f} Mbps < {threshold_dl:.2f} Mbps")
            log.warning(f"Below threshold: avg download {avg_dl:.2f} Mbps < {threshold_dl:.2f} Mbps")

        if min_dl < (threshold_dl * 0.5):
            anomalies.append(f"Severe degradation: min download {min_dl:.2f} Mbps")
            log.warning(f"Severe degradation detected: min download {min_dl:.2f} Mbps")

        if avg_dl < 100:
            anomalies.append(f"Performance drop: avg download {avg_dl:.2f} Mbps < 100 Mbps")
            log.warning(f"Performance drop detected: avg download {avg_dl:.2f} Mbps")

    if pings:
        avg_ping = mean(pings)
        max_ping = max(pings)

        if avg_ping > 20:
            anomalies.append(f"High latency: avg ping {avg_ping:.2f} ms")
            log.warning(f"High latency detected: avg ping {avg_ping:.2f} ms")

        if max_ping > 50:
            anomalies.append(f"Latency spike: max ping {max_ping:.2f} ms")
            log.warning(f"Latency spike detected: max ping {max_ping:.2f} ms")

    return anomalies

def emit_success_event(mode: str, date: str = None):
    """Send a success signal to EventBridge when aggregation completes."""
    detail = {"status": "success", "mode": mode}
    # if date:
    #     detail["date"] = date
    # events.put_events(
    #     Entries=[
    #         {
    #             "Source": "vd-speed-test.aggregator",
    #             "DetailType": "AggregationComplete",
    #             "Detail": json.dumps(detail),
    #             "EventBusName": "default"
    #         }
    #     ]
    # )
    log.info(f" Emitted success event for mode={mode}")

# --- Daily aggregation ---------------------------------------------------------
@log_execution
def aggregate_for_date(target_dt: datetime.datetime):
    """Aggregate all speed test results for the given IST date."""
    year = target_dt.strftime("%Y")
    month = target_dt.strftime("%Y%m")
    day = target_dt.strftime("%Y%m%d")
    prefix = f"year={year}/month={month}/day={day}/"
    log.info(f"Scanning prefix: {prefix}")

    downloads, uploads, pings = [], [], []
    servers, result_urls = [], []
    ips = set()
    connection_types = []
    count = 0
    errors_count = 0

    for key in list_objects(prefix):
        if not key.endswith(".json"):
            continue
        try:
            rec = read_json(key)
            dl = parse_mbps(rec.get("download_mbps"))
            ul = parse_mbps(rec.get("upload_mbps"))
            ping = parse_float(rec.get("ping_ms"))

            if dl is None or ul is None:
                errors_count += 1
                continue

            downloads.append(dl)
            uploads.append(ul)
            pings.append(ping)
            count += 1

            s_name = (rec.get("server_name") or "").strip()
            s_host = (rec.get("server_host") or "").strip()
            s_city = (rec.get("server_city") or "").strip()
            s_country = (rec.get("server_country") or "").strip()
            parts = [p for p in [s_name, s_host, s_city] if p]
            label = " â€“ ".join(parts)
            if s_country:
                label += f" ({s_country})"
            servers.append(label)

            url = rec.get("result_url")
            if isinstance(url, str) and url.startswith("http"):
                result_urls.append(url)

            ip = rec.get("public_ip")
            if isinstance(ip, str) and ip:
                ips.add(ip)
            
            conn_type = rec.get("connection_type")
            if conn_type:
                connection_types.append(conn_type)

        except Exception as e:
            log.warning(f"Skipping {key}: {e}")
            errors_count += 1

    if count == 0:
        log.warning(f"No data found for date {day}")
        return None

    anomalies = detect_anomalies(downloads, uploads, pings)

    expected_records = 96  # 15-min intervals
    completion_rate = (count / expected_records) * 100
    if count < expected_records * 0.8:
        log.warning(f"Low data completion: {count}/{expected_records} records ({completion_rate:.1f}%)")
        anomalies.append(f"Low completion rate: {completion_rate:.1f}%")

    if errors_count > 5:
        log.warning(f"High error rate: {errors_count} failed records")

    top_servers = [s for s, _ in Counter(servers).most_common(5)]
    top_connection_types = [ct for ct, _ in Counter(connection_types).most_common(3)]

    summary = {
        "date_ist": target_dt.strftime("%Y-%m-%d"),
        "records": count,
        "completion_rate": round(completion_rate, 1),
        "errors": errors_count,
        "overall": {
            "download_mbps": stats(downloads),
            "upload_mbps": stats(uploads),
            "ping_ms": stats(pings),
        },
        "servers_top": top_servers,
        "result_urls": result_urls,
        "public_ips": sorted(list(ips)),
        "connection_types": top_connection_types,
        "anomalies": anomalies,
        "threshold_mbps": EXPECTED_SPEED_MBPS,
    }

    log.info(f"Aggregated {count} records for {day} with {len(anomalies)} anomalies")
    return summary

@log_execution
def upload_summary(summary: dict, target_dt: datetime.datetime) -> str:
    """Upload the daily summary JSON back to S3."""
    year = target_dt.strftime("%Y")
    month = target_dt.strftime("%Y%m")
    day = target_dt.strftime("%Y%m%d")
    key = f"aggregated/year={year}/month={month}/day={day}/speed_test_summary.json"

    s3.put_object(
        Bucket=S3_BUCKET,
        Key=key,
        Body=json.dumps(summary, indent=2, ensure_ascii=False),
        ContentType="application/json",
    )
    log.info(f"Uploaded aggregated summary to s3://{S3_BUCKET}/{key}")
    return key

@log_execution
def run_daily(custom_date: str = None):
    """
    Run daily aggregation. If custom_date (YYYY-MM-DD) is provided, aggregate for that day.
    Otherwise, aggregate for the previous IST day.
    """
    if custom_date:
        try:
            target_date = datetime.datetime.strptime(custom_date, "%Y-%m-%d").date()
            log.info(f"Running manual backfill for date {custom_date}")
        except ValueError:
            log.error(f"Invalid date format: {custom_date}, expected YYYY-MM-DD")
            return {"message": f"Invalid date: {custom_date}", "records": 0}
    else:
        now_ist = datetime.datetime.now(TIMEZONE)
        target_date = (now_ist - datetime.timedelta(days=1)).date()

    target_dt = datetime.datetime.combine(target_date, datetime.time.min).replace(tzinfo=TIMEZONE)
    log.info(f"Aggregating for {target_dt.strftime('%Y-%m-%d')} IST")

    summary = aggregate_for_date(target_dt)
    if not summary:
        log.error("No records found for aggregation")
        return {"message": "No records found", "records": 0}

    key = upload_summary(summary, target_dt)
    result = {
        "message": "Daily aggregation complete",
        "records": summary["records"],
        "completion_rate": summary["completion_rate"],
        "errors": summary["errors"],
        "avg_download": summary["overall"]["download_mbps"]["avg"],
        "avg_upload": summary["overall"]["upload_mbps"]["avg"],
        "avg_ping": summary["overall"]["ping_ms"]["avg"],
        "unique_servers": summary.get("servers_top", []),
        "urls_count": len(summary.get("result_urls", [])),
        "unique_ips": summary.get("public_ips", []),
        "anomalies_count": len(summary.get("anomalies", [])),
        "s3_key": key,
    }
    log.info(f"Aggregation summary: {json.dumps(result, indent=2)}")
    emit_success_event("daily")      # at end of run_daily
    return result

# --- Hourly rollup (aggregate minute data into hourly summaries) --------------
@log_execution
def aggregate_hourly():
    """
    Aggregate all minute-level data for the previous hour into an hourly summary.
    """
    now_ist = datetime.datetime.now(TIMEZONE)
    # Target the previous completed hour
    target_hour = (now_ist - datetime.timedelta(hours=1)).replace(minute=0, second=0, microsecond=0)
    
    year = target_hour.strftime("%Y")
    month = target_hour.strftime("%Y%m")
    day = target_hour.strftime("%Y%m%d")
    hour = target_hour.strftime("%Y%m%d%H")
    
    prefix = f"year={year}/month={month}/day={day}/hour={hour}/"
    log.info(f"Scanning prefix for hourly aggregation: {prefix}")

    downloads, uploads, pings = [], [], []
    servers = []
    ips = set()
    connection_types = []
    count = 0
    errors_count = 0

    for key in list_objects(prefix):
        if not key.endswith(".json"):
            continue
        try:
            rec = read_json(key)
            dl = parse_mbps(rec.get("download_mbps"))
            ul = parse_mbps(rec.get("upload_mbps"))
            ping = parse_float(rec.get("ping_ms"))

            if dl is None or ul is None:
                errors_count += 1
                continue

            downloads.append(dl)
            uploads.append(ul)
            pings.append(ping)
            count += 1

            s_name = (rec.get("server_name") or "").strip()
            s_host = (rec.get("server_host") or "").strip()
            s_city = (rec.get("server_city") or "").strip()
            s_country = (rec.get("server_country") or "").strip()
            parts = [p for p in [s_name, s_host, s_city] if p]
            label = " â€“ ".join(parts)
            if s_country:
                label += f" ({s_country})"
            servers.append(label)

            ip = rec.get("public_ip")
            if isinstance(ip, str) and ip:
                ips.add(ip)
            
            conn_type = rec.get("connection_type")
            if conn_type:
                connection_types.append(conn_type)

        except Exception as e:
            log.warning(f"Skipping {key}: {e}")
            errors_count += 1

    if count == 0:
        log.warning(f"No data found for hour {hour}")
        return None

    anomalies = detect_anomalies(downloads, uploads, pings)
    
    # Expect 4 records per hour (15-min intervals: 00, 15, 30, 45)
    # But proceed with partial data (1-4 files) as they become available
    expected_records = 4
    completion_rate = (count / expected_records) * 100
    if count < expected_records:
        log.info(f"Partial hour data: {count}/{expected_records} records ({completion_rate:.1f}%)")
        # Note: We still aggregate with whatever data is available

    top_servers = [s for s, _ in Counter(servers).most_common(3)]
    top_connection_types = [ct for ct, _ in Counter(connection_types).most_common(3)]

    summary = {
        "hour_ist": target_hour.strftime("%Y-%m-%d %H:00"),
        "records": count,
        "completion_rate": round(completion_rate, 1),
        "errors": errors_count,
        "overall": {
            "download_mbps": stats(downloads),
            "upload_mbps": stats(uploads),
            "ping_ms": stats(pings),
        },
        "servers_top": top_servers,
        "public_ips": sorted(list(ips)),
        "connection_types": top_connection_types,
        "anomalies": anomalies,
        "threshold_mbps": EXPECTED_SPEED_MBPS,
    }

    # Upload to hourly bucket
    key = f"aggregated/year={year}/month={month}/day={day}/hour={hour}/speed_test_summary.json"
    s3.put_object(
        Bucket=S3_BUCKET_HOURLY,
        Key=key,
        Body=json.dumps(summary, indent=2, ensure_ascii=False),
        ContentType="application/json",
    )
    log.info(f"Hourly summary uploaded to s3://{S3_BUCKET_HOURLY}/{key}")
    emit_success_event("hourly")
    return summary

# --- Weekly rollup (last completed Mon..Sun) ----------------------------------
@log_execution
def aggregate_weekly():
    """
    Aggregate all available daily summaries for the current or last completed week (Monâ€“Sun).
    """
    today_ist = datetime.datetime.now(TIMEZONE).date()
    this_monday = today_ist - datetime.timedelta(days=today_ist.weekday())  # Monday of this week
    this_sunday = this_monday + datetime.timedelta(days=6)

    # Optionally, if today is Monday, fallback to the previous week
    if today_ist == this_monday:
        last_monday = this_monday - datetime.timedelta(days=7)
        this_sunday = last_monday + datetime.timedelta(days=6)
        this_monday = last_monday

    daily_summaries = []
    d = this_monday
    while d <= this_sunday:
        y, m, dd = d.strftime("%Y"), d.strftime("%Y%m"), d.strftime("%Y%m%d")
        key = f"aggregated/year={y}/month={m}/day={dd}/speed_test_summary.json"
        try:
            obj = s3.get_object(Bucket=S3_BUCKET, Key=key)
            daily_summaries.append(json.loads(obj["Body"].read()))
        except s3.exceptions.NoSuchKey:
            log.warning(f"Missing daily summary for {dd}")
        except Exception as e:
            log.warning(f"Error reading {key}: {e}")
        d += datetime.timedelta(days=1)

    if not daily_summaries:
        log.warning("No daily summaries found for this week")
        return None

    all_connection_types = []
    for ds in daily_summaries:
        all_connection_types.extend(ds.get("connection_types", []))
    top_connection_types = [ct for ct, _ in Counter(all_connection_types).most_common(3)] if all_connection_types else []

    summary = {
        "week_start": str(this_monday),
        "week_end": str(this_sunday),
        "days": len(daily_summaries),
        "avg_download": round(mean([x["overall"]["download_mbps"]["avg"] for x in daily_summaries]), 2),
        "avg_upload": round(mean([x["overall"]["upload_mbps"]["avg"] for x in daily_summaries]), 2),
        "avg_ping": round(mean([x["overall"]["ping_ms"]["avg"] for x in daily_summaries]), 2),
        "connection_types": top_connection_types,
    }

    week_label = f"{this_monday.strftime('%YW%W')}"
    key = f"aggregated/year={this_monday.year}/week={week_label}/speed_test_summary.json"
    s3.put_object(
        Bucket=S3_BUCKET_WEEKLY,
        Key=key,
        Body=json.dumps(summary, indent=2),
        ContentType="application/json",
    )
    log.info(f"Weekly summary uploaded to s3://{S3_BUCKET_WEEKLY}/{key}")
    emit_success_event("weekly")     # at end of aggregate_weekly
    return summary

# --- Monthly rollup (previous calendar month) ---------------------------------
@log_execution
def aggregate_monthly():
    """Aggregate all daily summaries for the current month so far."""
    today = datetime.datetime.now(TIMEZONE).date()
    first_day = today.replace(day=1)
    _, last_day_num = monthrange(today.year, today.month)
    last_day = today  # up to today (or first_day + last_day_num for full month)
    month_tag = today.strftime("%Y%m")

    summaries = []
    d = first_day
    while d <= last_day:
        y, m, dd = d.strftime("%Y"), d.strftime("%Y%m"), d.strftime("%Y%m%d")
        key = f"aggregated/year={y}/month={m}/day={dd}/speed_test_summary.json"
        try:
            obj = s3.get_object(Bucket=S3_BUCKET, Key=key)
            summaries.append(json.loads(obj["Body"].read()))
        except Exception:
            pass
        d += datetime.timedelta(days=1)

    if not summaries:
        log.warning(f"No daily summaries found for month {month_tag}")
        return None

    all_connection_types = []
    for ds in summaries:
        all_connection_types.extend(ds.get("connection_types", []))
    top_connection_types = [ct for ct, _ in Counter(all_connection_types).most_common(3)] if all_connection_types else []

    summary = {
        "month": month_tag,
        "days": len(summaries),
        "avg_download": round(mean([x["overall"]["download_mbps"]["avg"] for x in summaries]), 2),
        "avg_upload": round(mean([x["overall"]["upload_mbps"]["avg"] for x in summaries]), 2),
        "avg_ping": round(mean([x["overall"]["ping_ms"]["avg"] for x in summaries]), 2),
        "connection_types": top_connection_types,
    }

    key = f"aggregated/year={first_day.year}/month={month_tag}/speed_test_summary.json"
    s3.put_object(
        Bucket=S3_BUCKET_MONTHLY,
        Key=key,
        Body=json.dumps(summary, indent=2),
        ContentType="application/json",
    )
    log.info(f"Monthly summary uploaded to s3://{S3_BUCKET_MONTHLY}/{key}")
    emit_success_event("monthly")     # at end of aggregate_monthly
    return summary

# --- Yearly rollup (previous calendar year) -----------------------------------
@log_execution
def aggregate_yearly():
    """Aggregate all monthly summaries for the current year (YTD)."""
    now_ist = datetime.datetime.now(TIMEZONE).date()
    current_year = now_ist.year
    summaries = []

    # Look for all monthly summaries from Jan up to current month
    for m in range(1, now_ist.month + 1):
        month_str = f"{current_year}{m:02d}"
        key = f"aggregated/year={current_year}/month={month_str}/speed_test_summary.json"
        try:
            obj = s3.get_object(Bucket=S3_BUCKET_MONTHLY, Key=key)
            summaries.append(json.loads(obj["Body"].read()))
        except Exception as e:
            log.warning(f"Skipping missing month {month_str}: {e}")
            continue

    if not summaries:
        log.warning(f"No monthly summaries found for {current_year}")
        return None

    all_connection_types = []
    for ms in summaries:
        all_connection_types.extend(ms.get("connection_types", []))
    top_connection_types = [ct for ct, _ in Counter(all_connection_types).most_common(3)] if all_connection_types else []

    summary = {
        "year": current_year,
        "months_aggregated": len(summaries),
        "avg_download": round(mean([x["avg_download"] for x in summaries]), 2),
        "avg_upload": round(mean([x["avg_upload"] for x in summaries]), 2),
        "avg_ping": round(mean([x["avg_ping"] for x in summaries]), 2),
        "connection_types": top_connection_types,
    }

    key = f"aggregated/year={current_year}/speed_test_summary.json"
    s3.put_object(
        Bucket=S3_BUCKET_YEARLY,
        Key=key,
        Body=json.dumps(summary, indent=2),
        ContentType="application/json",
    )
    log.info(f"Year-to-date summary uploaded to s3://{S3_BUCKET_YEARLY}/{key}")
    emit_success_event("yearly")     # at end of aggregate_yearly
    return summary

# --- Lambda handler -----------------------------------------------------------
# ---------------------------------------------------------------------------
# ðŸ”€ Extended Lambda Handler (supporting "mode": hourly|daily|weekly|monthly|yearly)
# ---------------------------------------------------------------------------
def lambda_handler(event, context):
    mode = (event or {}).get("mode") if isinstance(event, dict) else None
    log.info(f"Lambda trigger â€” mode={mode or 'daily'}")
    try:
        if mode == "hourly":
            result = aggregate_hourly()
        elif mode == "weekly":
            result = aggregate_weekly()
        elif mode == "monthly":
            result = aggregate_monthly()
        elif mode == "yearly":
            result = aggregate_yearly()
        else:
            custom_date = (event or {}).get("date")
            result = run_daily(custom_date)


        return {
            "statusCode": 200,
            "headers": {"Content-Type": "application/json"},
            "body": json.dumps({"message": "ok", "mode": mode or "daily", "result": result}, indent=2),
        }
    except Exception as e:
        log.exception(f"Lambda failed in {mode or 'daily'} mode: {e}")
        return {
            "statusCode": 500,
            "headers": {"Content-Type": "application/json"},
            "body": json.dumps({"error": str(e)}),
        }

if __name__ == "__main__":
    # Local quick tests (uncomment the one you want)
    # print(json.dumps(run_daily(), indent=2))
    # print(json.dumps(aggregate_hourly(), indent=2))
    # print(json.dumps(aggregate_weekly(), indent=2))
    # print(json.dumps(aggregate_monthly(), indent=2))
    # print(json.dumps(aggregate_yearly(), indent=2))
    pass

